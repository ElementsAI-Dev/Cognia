# RAG æ¨¡å—æ”¹è¿›è®¡åˆ’

åŸºäº AI SDK æ–‡æ¡£å’Œç°æœ‰ä»£ç åˆ†æï¼Œæœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜æ‰€æœ‰æ”¹è¿›æ–¹æ¡ˆã€‚

---

## ğŸ“‹ ç›®å½•

1. [ç°æœ‰å®ç°åˆ†æ](#1-ç°æœ‰å®ç°åˆ†æ)
2. [æ”¹è¿›æ–¹æ¡ˆæ€»è§ˆ](#2-æ”¹è¿›æ–¹æ¡ˆæ€»è§ˆ)
3. [è¯¦ç»†æ”¹è¿›æ–¹æ¡ˆ](#3-è¯¦ç»†æ”¹è¿›æ–¹æ¡ˆ)
4. [æ–‡ä»¶å˜æ›´æ¸…å•](#4-æ–‡ä»¶å˜æ›´æ¸…å•)
5. [å®æ–½é¡ºåº](#5-å®æ–½é¡ºåº)

---

## 1. ç°æœ‰å®ç°åˆ†æ

### 1.1 å½“å‰æ¶æ„

```
lib/ai/
â”œâ”€â”€ embedding.ts           # åµŒå…¥ç”Ÿæˆ (ä½¿ç”¨ AI SDK embed/embedMany)
â”œâ”€â”€ chunking.ts            # æ–‡æ¡£åˆ†å— (5ç§ç­–ç•¥)
â”œâ”€â”€ rag.ts                 # åŸºç¡€ RAG æœåŠ¡
â””â”€â”€ rag/
    â”œâ”€â”€ index.ts           # æ¨¡å—å¯¼å‡º
    â”œâ”€â”€ rag-pipeline.ts    # é«˜çº§ RAG ç®¡é“
    â”œâ”€â”€ hybrid-search.ts   # BM25 + Vector æ··åˆæœç´¢
    â”œâ”€â”€ reranker.ts        # é‡æ’åº (LLM/Cohere/Heuristics/MMR)
    â”œâ”€â”€ query-expansion.ts # æŸ¥è¯¢æ‰©å±• (HyDE/Variants/Synonyms)
    â””â”€â”€ contextual-retrieval.ts  # ä¸Šä¸‹æ–‡æ£€ç´¢
```

### 1.2 ç°æœ‰é—®é¢˜

| é—®é¢˜ | ä½ç½® | å½±å“ |
|------|------|------|
| å¤šå¤„é‡å¤å®ç° `cosineSimilarity` | embedding.ts, rag-pipeline.ts, reranker.ts | ä»£ç å†—ä½™ï¼Œä¸ä¸€è‡´ |
| ç¼ºå°‘ `maxParallelCalls` æ§åˆ¶ | embedding.ts | å¤§é‡è¯·æ±‚æ—¶å¯èƒ½è§¦å‘é™æµ |
| Provider æ”¯æŒæœ‰é™ | embedding.ts | ç¼ºå°‘ Amazon Bedrock, Azure, Voyage |
| ç¼ºå°‘ embedding model ä¸­é—´ä»¶ | embedding.ts | æ— æ³•è®¾ç½®é»˜è®¤é…ç½® |
| RAG æœªä½œä¸º Tool é›†æˆ | æ—  | æ— æ³•åœ¨ streamText ä¸­è‡ªåŠ¨è°ƒç”¨ |
| ç¼“å­˜ä»…å†…å­˜å­˜å‚¨ | embedding.ts | é‡å¯åä¸¢å¤± |
| ç¼ºå°‘é«˜çº§æŸ¥æ‰¾ API | æ—  | ä½¿ç”¨ä¸ä¾¿ |

---

## 2. æ”¹è¿›æ–¹æ¡ˆæ€»è§ˆ

| åºå· | æ”¹è¿›é¡¹ | ä¼˜å…ˆçº§ | å¤æ‚åº¦ | å½±å“èŒƒå›´ |
|------|--------|--------|--------|----------|
| 1 | ä½¿ç”¨ AI SDK åŸç”Ÿ `cosineSimilarity` | é«˜ | ä½ | 3ä¸ªæ–‡ä»¶ |
| 2 | æ·»åŠ  `maxParallelCalls` æ”¯æŒ | é«˜ | ä½ | 1ä¸ªæ–‡ä»¶ |
| 3 | ä½¿ç”¨ `wrapEmbeddingModel` ä¸­é—´ä»¶ | ä¸­ | ä¸­ | 1ä¸ªæ–‡ä»¶ |
| 4 | åˆ›å»º RAG Tool ç”¨äº streamText | é«˜ | ä¸­ | æ–°å»ºæ–‡ä»¶ |
| 5 | æ·»åŠ æ›´å¤š embedding provider | ä¸­ | ä¸­ | 1ä¸ªæ–‡ä»¶ |
| 6 | ä¼˜åŒ–ç¼“å­˜ç­–ç•¥ï¼ˆæŒä¹…åŒ–ï¼‰ | ä¸­ | ä¸­ | æ–°å»ºæ–‡ä»¶ |
| 7 | æ·»åŠ  `findRelevantContent` API | ä¸­ | ä½ | æ–°å»ºæ–‡ä»¶ |
| 8 | å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯• | é«˜ | ä½ | 1ä¸ªæ–‡ä»¶ |
| 9 | æ·»åŠ  `providerOptions` æ”¯æŒ | ä¸­ | ä¸­ | 1ä¸ªæ–‡ä»¶ |
| 10 | æ›´æ–°æµ‹è¯•ç”¨ä¾‹ | é«˜ | ä¸­ | å¤šä¸ªæ–‡ä»¶ |

---

## 3. è¯¦ç»†æ”¹è¿›æ–¹æ¡ˆ

### 3.1 ä½¿ç”¨ AI SDK åŸç”Ÿ `cosineSimilarity`

**èƒŒæ™¯**: AI SDK æä¾›äº† `cosineSimilarity` å‡½æ•°ï¼Œå½“å‰ä»£ç æœ‰å¤šå¤„è‡ªå®šä¹‰å®ç°ã€‚

**AI SDK API**:

```typescript
import { cosineSimilarity } from 'ai';

// è¿”å› -1 åˆ° 1 ä¹‹é—´çš„æ•°å€¼ï¼Œ1 è¡¨ç¤ºå®Œå…¨ç›¸ä¼¼
const similarity = cosineSimilarity(embedding1, embedding2);
```

**éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**:

#### `lib/ai/embedding.ts` (320-337è¡Œ)

```typescript
// åˆ é™¤è‡ªå®šä¹‰å®ç°:
// export function cosineSimilarity(a: number[], b: number[]): number { ... }

// æ”¹ä¸ºä» AI SDK é‡æ–°å¯¼å‡º:
import { cosineSimilarity as aiCosineSimilarity } from 'ai';
export const cosineSimilarity = aiCosineSimilarity;
```

#### `lib/ai/rag/rag-pipeline.ts` (471-481è¡Œ)

```typescript
// åˆ é™¤ç§æœ‰æ–¹æ³• cosineSimilarity
// ä½¿ç”¨å¯¼å…¥:
import { cosineSimilarity } from '@/lib/ai/embedding';
```

#### `lib/ai/rag/reranker.ts` (296-306è¡Œ)

```typescript
// åˆ é™¤å±€éƒ¨å‡½æ•° cosineSimilarity
// ä½¿ç”¨å¯¼å…¥:
import { cosineSimilarity } from '@/lib/ai/embedding';
```

---

### 3.2 æ·»åŠ  `maxParallelCalls` æ”¯æŒ

**èƒŒæ™¯**: AI SDK çš„ `embedMany` æ”¯æŒ `maxParallelCalls` å‚æ•°æ§åˆ¶å¹¶å‘è¯·æ±‚æ•°ã€‚

**AI SDK API**:

```typescript
const { embeddings, usage } = await embedMany({
  model: openai.textEmbeddingModel('text-embedding-3-small'),
  values: ['text1', 'text2', 'text3'],
  maxParallelCalls: 2, // é™åˆ¶å¹¶è¡Œè¯·æ±‚æ•°ï¼Œé»˜è®¤ Infinity
});
```

**ä¿®æ”¹ `lib/ai/embedding.ts`**:

```typescript
export interface EmbeddingConfig {
  provider: ProviderName;
  model?: string;
  apiKey: string;
  baseURL?: string;
  dimensions?: number;
  cache?: EmbeddingCache;
  // æ–°å¢å­—æ®µ:
  maxParallelCalls?: number;  // æ§åˆ¶å¹¶è¡Œè¯·æ±‚æ•°
  maxRetries?: number;        // æœ€å¤§é‡è¯•æ¬¡æ•°
  abortSignal?: AbortSignal;  // å–æ¶ˆä¿¡å·
}

export async function generateEmbeddings(
  texts: string[],
  config: EmbeddingConfig
): Promise<BatchEmbeddingResult> {
  // ... ç¼“å­˜é€»è¾‘ ...
  
  const result = await embedMany({
    model,
    values: textsToEmbed.map((t) => t.text),
    maxParallelCalls: config.maxParallelCalls ?? 5,  // æ–°å¢
    maxRetries: config.maxRetries ?? 2,              // æ–°å¢
    abortSignal: config.abortSignal,                 // æ–°å¢
  });
  
  // ... å…¶ä½™é€»è¾‘ ...
}
```

---

### 3.3 ä½¿ç”¨ `wrapEmbeddingModel` ä¸­é—´ä»¶

**èƒŒæ™¯**: AI SDK æä¾› `wrapEmbeddingModel` å’Œ `defaultEmbeddingSettingsMiddleware` ç”¨äºè®¾ç½®é»˜è®¤é…ç½®ã€‚

**AI SDK API**:

```typescript
import { wrapEmbeddingModel, defaultEmbeddingSettingsMiddleware } from 'ai';

const embeddingModelWithDefaults = wrapEmbeddingModel({
  model: openai.embedding('text-embedding-3-small'),
  middleware: defaultEmbeddingSettingsMiddleware({
    settings: {
      providerOptions: {
        openai: {
          dimensions: 256,
        },
      },
    },
  }),
});
```

**æ–°å¢ `lib/ai/embedding-model-factory.ts`**:

```typescript
/**
 * Embedding Model Factory with Default Settings
 * 
 * Provides wrapped embedding models with default configurations
 */

import { 
  wrapEmbeddingModel, 
  defaultEmbeddingSettingsMiddleware,
  type EmbeddingModel,
} from 'ai';
import { createOpenAI } from '@ai-sdk/openai';
import { createGoogleGenerativeAI } from '@ai-sdk/google';
import { createCohere } from '@ai-sdk/cohere';
import { createMistral } from '@ai-sdk/mistral';
import { createAmazonBedrock } from '@ai-sdk/amazon-bedrock';
import { createAzure } from '@ai-sdk/azure';

export interface EmbeddingModelFactoryConfig {
  provider: string;
  model: string;
  apiKey: string;
  baseURL?: string;
  // Provider-specific defaults
  defaults?: {
    dimensions?: number;
    // OpenAI specific
    user?: string;
    // Google specific
    taskType?: 'RETRIEVAL_DOCUMENT' | 'RETRIEVAL_QUERY' | 'SEMANTIC_SIMILARITY' | 'CLASSIFICATION' | 'CLUSTERING';
    // Cohere specific
    inputType?: 'search_document' | 'search_query' | 'classification' | 'clustering';
    truncate?: 'NONE' | 'START' | 'END';
  };
}

export function createEmbeddingModelWithDefaults(
  config: EmbeddingModelFactoryConfig
): EmbeddingModel<string> {
  const baseModel = getBaseEmbeddingModel(config);
  
  if (!config.defaults) {
    return baseModel;
  }

  const providerOptions = buildProviderOptions(config);
  
  return wrapEmbeddingModel({
    model: baseModel,
    middleware: defaultEmbeddingSettingsMiddleware({
      settings: {
        providerOptions,
      },
    }),
  });
}

function getBaseEmbeddingModel(config: EmbeddingModelFactoryConfig): EmbeddingModel<string> {
  const { provider, model, apiKey, baseURL } = config;
  
  switch (provider) {
    case 'openai': {
      const openai = createOpenAI({ apiKey, baseURL });
      return openai.embedding(model);
    }
    case 'google': {
      const google = createGoogleGenerativeAI({ apiKey });
      return google.textEmbeddingModel(model);
    }
    case 'cohere': {
      const cohere = createCohere({ apiKey });
      return cohere.embedding(model);
    }
    case 'mistral': {
      const mistral = createMistral({ apiKey });
      return mistral.embedding(model);
    }
    case 'amazon-bedrock': {
      const bedrock = createAmazonBedrock({});
      return bedrock.embedding(model);
    }
    case 'azure': {
      const azure = createAzure({ apiKey, baseURL });
      return azure.embedding(model);
    }
    default:
      throw new Error(`Unsupported embedding provider: ${provider}`);
  }
}

function buildProviderOptions(config: EmbeddingModelFactoryConfig): Record<string, unknown> {
  const { provider, defaults } = config;
  if (!defaults) return {};
  
  switch (provider) {
    case 'openai':
    case 'azure':
      return {
        openai: {
          dimensions: defaults.dimensions,
          user: defaults.user,
        },
      };
    case 'google':
      return {
        google: {
          outputDimensionality: defaults.dimensions,
          taskType: defaults.taskType,
        },
      };
    case 'cohere':
      return {
        cohere: {
          inputType: defaults.inputType,
          truncate: defaults.truncate,
        },
      };
    case 'amazon-bedrock':
      return {
        bedrock: {
          dimensions: defaults.dimensions,
          normalize: true,
        },
      };
    default:
      return {};
  }
}
```

---

### 3.4 åˆ›å»º RAG Tool ç”¨äº streamText

**èƒŒæ™¯**: AI SDK æ”¯æŒé€šè¿‡ `tool()` å®šä¹‰å·¥å…·ï¼Œå¯åœ¨ `streamText` ä¸­è‡ªåŠ¨è°ƒç”¨ã€‚

**AI SDK API**:

```typescript
import { tool, streamText, UIMessage, stepCountIs } from 'ai';
import { z } from 'zod';

const result = streamText({
  model: openai('gpt-4o'),
  messages,
  stopWhen: stepCountIs(5),
  tools: {
    getInformation: tool({
      description: 'Get information from knowledge base',
      parameters: z.object({
        question: z.string().describe('The question to search'),
      }),
      execute: async ({ question }) => findRelevantContent(question),
    }),
  },
});
```

**æ–°å»º `lib/ai/rag/rag-tools.ts`**:

```typescript
/**
 * RAG Tools for AI SDK Integration
 * 
 * Provides tool definitions for use with streamText
 */

import { tool } from 'ai';
import { z } from 'zod';
import type { RAGPipeline } from './rag-pipeline';

export interface RAGToolsConfig {
  pipeline: RAGPipeline;
  collectionName: string;
  /** Maximum results to return */
  topK?: number;
  /** Minimum similarity threshold */
  similarityThreshold?: number;
}

/**
 * Create RAG tools for use with streamText
 */
export function createRAGTools(config: RAGToolsConfig) {
  const { pipeline, collectionName, topK = 5, similarityThreshold = 0.5 } = config;

  return {
    /**
     * Get information from knowledge base
     */
    getInformation: tool({
      description: `Search the knowledge base for relevant information to answer questions. 
      Always use this tool before answering questions that might require specific knowledge.`,
      parameters: z.object({
        question: z.string().describe('The question or topic to search for'),
      }),
      execute: async ({ question }) => {
        try {
          const context = await pipeline.retrieve(collectionName, question);
          
          if (context.documents.length === 0) {
            return 'No relevant information found in the knowledge base.';
          }
          
          // Filter by threshold and limit
          const relevantDocs = context.documents
            .filter(d => d.rerankScore >= similarityThreshold)
            .slice(0, topK);
          
          if (relevantDocs.length === 0) {
            return 'No sufficiently relevant information found.';
          }
          
          // Format results
          return relevantDocs.map((doc, i) => 
            `[Source ${i + 1}] (Score: ${doc.rerankScore.toFixed(2)})\n${doc.content}`
          ).join('\n\n---\n\n');
        } catch (error) {
          console.error('RAG retrieval error:', error);
          return 'Error retrieving information from knowledge base.';
        }
      },
    }),

    /**
     * Add resource to knowledge base
     */
    addResource: tool({
      description: `Add new information to the knowledge base. 
      Use this when the user provides information that should be remembered.`,
      parameters: z.object({
        content: z.string().describe('The content to add to the knowledge base'),
        title: z.string().optional().describe('Optional title for the content'),
      }),
      execute: async ({ content, title }) => {
        try {
          const result = await pipeline.indexDocument(content, {
            collectionName,
            documentId: `doc-${Date.now()}-${Math.random().toString(36).slice(2, 9)}`,
            documentTitle: title,
          });
          
          if (result.success) {
            return `Successfully added ${result.chunksCreated} chunks to the knowledge base.`;
          } else {
            return `Failed to add content: ${result.error}`;
          }
        } catch (error) {
          console.error('RAG indexing error:', error);
          return 'Error adding content to knowledge base.';
        }
      },
    }),

    /**
     * Search with specific filters
     */
    searchWithFilters: tool({
      description: 'Search the knowledge base with specific filters like date range or category',
      parameters: z.object({
        query: z.string().describe('The search query'),
        filters: z.object({
          category: z.string().optional().describe('Filter by category'),
          startDate: z.string().optional().describe('Filter by start date (ISO format)'),
          endDate: z.string().optional().describe('Filter by end date (ISO format)'),
        }).optional(),
      }),
      execute: async ({ query, filters }) => {
        try {
          const context = await pipeline.retrieve(collectionName, query);
          
          let results = context.documents;
          
          // Apply filters if provided
          if (filters) {
            if (filters.category) {
              results = results.filter(d => 
                d.metadata?.category === filters.category
              );
            }
            if (filters.startDate) {
              const start = new Date(filters.startDate).getTime();
              results = results.filter(d => {
                const docDate = d.metadata?.createdAt;
                return docDate && new Date(String(docDate)).getTime() >= start;
              });
            }
            if (filters.endDate) {
              const end = new Date(filters.endDate).getTime();
              results = results.filter(d => {
                const docDate = d.metadata?.createdAt;
                return docDate && new Date(String(docDate)).getTime() <= end;
              });
            }
          }
          
          if (results.length === 0) {
            return 'No results found matching the filters.';
          }
          
          return results.slice(0, topK).map((doc, i) => 
            `[Result ${i + 1}]\n${doc.content}`
          ).join('\n\n---\n\n');
        } catch (error) {
          console.error('Filtered search error:', error);
          return 'Error performing filtered search.';
        }
      },
    }),
  };
}

/**
 * Create a simple retrieval tool for basic RAG
 */
export function createSimpleRetrievalTool(
  findRelevantContent: (query: string) => Promise<Array<{ content: string; similarity: number }>>
) {
  return tool({
    description: 'Search the knowledge base for relevant information',
    parameters: z.object({
      question: z.string().describe('The question to search for'),
    }),
    execute: async ({ question }) => {
      const results = await findRelevantContent(question);
      if (results.length === 0) {
        return 'No relevant information found.';
      }
      return results.map((r, i) => 
        `[${i + 1}] (similarity: ${r.similarity.toFixed(2)}): ${r.content}`
      ).join('\n\n');
    },
  });
}
```

---

### 3.5 æ·»åŠ æ›´å¤š Embedding Provider æ”¯æŒ

**èƒŒæ™¯**: å½“å‰ä»…æ”¯æŒ OpenAI, Google, Cohere, Mistral, Ollamaã€‚éœ€è¦æ·»åŠ  Amazon Bedrock, Azure, Voyage AIã€‚

**ä¿®æ”¹ `lib/ai/embedding.ts`**:

```typescript
import { createAmazonBedrock } from '@ai-sdk/amazon-bedrock';
import { createAzure } from '@ai-sdk/azure';
// Voyage AI æ˜¯ç¤¾åŒº provider
// import { voyage } from 'voyage-ai-provider';

export type EmbeddingProvider = 
  | 'openai' 
  | 'google' 
  | 'cohere' 
  | 'mistral' 
  | 'ollama'
  | 'amazon-bedrock'  // æ–°å¢
  | 'azure'           // æ–°å¢
  | 'voyage';         // æ–°å¢

export const defaultEmbeddingModels: Partial<Record<EmbeddingProvider, string>> = {
  openai: 'text-embedding-3-small',
  google: 'text-embedding-004',
  cohere: 'embed-english-v3.0',
  mistral: 'mistral-embed',
  ollama: 'nomic-embed-text',
  // æ–°å¢:
  'amazon-bedrock': 'amazon.titan-embed-text-v2:0',
  'azure': 'text-embedding-3-small',
  'voyage': 'voyage-3',
};

export interface EmbeddingConfig {
  provider: EmbeddingProvider;
  model?: string;
  apiKey: string;
  baseURL?: string;
  dimensions?: number;
  cache?: EmbeddingCache;
  maxParallelCalls?: number;
  maxRetries?: number;
  abortSignal?: AbortSignal;
  // Provider-specific options:
  providerOptions?: {
    // Azure specific
    resourceName?: string;
    apiVersion?: string;
    // Amazon Bedrock specific
    region?: string;
    // Cohere specific
    inputType?: 'search_document' | 'search_query' | 'classification' | 'clustering';
    // Google specific
    taskType?: string;
  };
}

function getEmbeddingModel(config: EmbeddingConfig) {
  const { provider, model, apiKey, baseURL, providerOptions } = config;

  switch (provider) {
    // ... ç°æœ‰ cases ...
    
    case 'amazon-bedrock': {
      const bedrock = createAmazonBedrock({
        region: providerOptions?.region || 'us-east-1',
      });
      const modelId = model || defaultEmbeddingModels['amazon-bedrock']!;
      return bedrock.embedding(modelId);
    }
    
    case 'azure': {
      const azure = createAzure({
        resourceName: providerOptions?.resourceName,
        apiKey,
        apiVersion: providerOptions?.apiVersion,
      });
      const modelId = model || defaultEmbeddingModels['azure']!;
      return azure.embedding(modelId);
    }
    
    case 'voyage': {
      // æ³¨æ„: voyage-ai-provider æ˜¯ç¤¾åŒºåŒ…ï¼Œéœ€è¦å•ç‹¬å®‰è£…
      throw new Error('Voyage AI provider requires voyage-ai-provider package');
    }
    
    default:
      throw new Error(`Embedding not supported for provider: ${provider}`);
  }
}
```

---

### 3.6 ä¼˜åŒ–ç¼“å­˜ç­–ç•¥ï¼ˆæŒä¹…åŒ–ï¼‰

**æ–°å»º `lib/ai/embedding-cache.ts`**:

```typescript
/**
 * Embedding Cache with Persistence Support
 * 
 * Provides LRU cache with optional IndexedDB persistence
 */

import { db } from '@/lib/db';

export interface EmbeddingCache {
  get(key: string): number[] | undefined;
  set(key: string, embedding: number[]): void;
  has(key: string): boolean;
  clear(): void;
  size(): number;
}

export interface PersistentEmbeddingCache extends EmbeddingCache {
  persist(): Promise<void>;
  load(): Promise<void>;
  getStats(): { hits: number; misses: number; size: number };
}

/**
 * Create an in-memory LRU cache (existing implementation)
 */
export function createEmbeddingCache(maxSize: number = 1000): EmbeddingCache {
  const cache = new Map<string, { embedding: number[]; accessTime: number }>();

  const evictOldest = () => {
    if (cache.size <= maxSize) return;
    
    let oldestKey: string | null = null;
    let oldestTime = Infinity;
    
    for (const [key, value] of cache.entries()) {
      if (value.accessTime < oldestTime) {
        oldestTime = value.accessTime;
        oldestKey = key;
      }
    }
    
    if (oldestKey) {
      cache.delete(oldestKey);
    }
  };

  return {
    get(key: string): number[] | undefined {
      const entry = cache.get(key);
      if (entry) {
        entry.accessTime = Date.now();
        return entry.embedding;
      }
      return undefined;
    },
    set(key: string, embedding: number[]): void {
      cache.set(key, { embedding, accessTime: Date.now() });
      evictOldest();
    },
    has(key: string): boolean {
      return cache.has(key);
    },
    clear(): void {
      cache.clear();
    },
    size(): number {
      return cache.size;
    },
  };
}

/**
 * Create a persistent cache backed by IndexedDB
 */
export function createPersistentEmbeddingCache(
  maxSize: number = 10000
): PersistentEmbeddingCache {
  const memoryCache = new Map<string, { embedding: number[]; accessTime: number }>();
  let hits = 0;
  let misses = 0;

  const evictOldest = () => {
    if (memoryCache.size <= maxSize) return;
    
    let oldestKey: string | null = null;
    let oldestTime = Infinity;
    
    for (const [key, value] of memoryCache.entries()) {
      if (value.accessTime < oldestTime) {
        oldestTime = value.accessTime;
        oldestKey = key;
      }
    }
    
    if (oldestKey) {
      memoryCache.delete(oldestKey);
    }
  };

  return {
    get(key: string): number[] | undefined {
      const entry = memoryCache.get(key);
      if (entry) {
        entry.accessTime = Date.now();
        hits++;
        return entry.embedding;
      }
      misses++;
      return undefined;
    },
    
    set(key: string, embedding: number[]): void {
      memoryCache.set(key, { embedding, accessTime: Date.now() });
      evictOldest();
    },
    
    has(key: string): boolean {
      return memoryCache.has(key);
    },
    
    clear(): void {
      memoryCache.clear();
      hits = 0;
      misses = 0;
    },
    
    size(): number {
      return memoryCache.size;
    },
    
    async persist(): Promise<void> {
      try {
        const entries = Array.from(memoryCache.entries()).map(([key, value]) => ({
          key,
          embedding: value.embedding,
          accessTime: value.accessTime,
        }));
        
        // ä½¿ç”¨ Dexie æ‰¹é‡å†™å…¥
        await db.embeddingCache?.bulkPut(entries);
      } catch (error) {
        console.warn('Failed to persist embedding cache:', error);
      }
    },
    
    async load(): Promise<void> {
      try {
        const entries = await db.embeddingCache?.toArray();
        if (entries) {
          for (const { key, embedding, accessTime } of entries) {
            memoryCache.set(key, { embedding, accessTime });
          }
        }
      } catch (error) {
        console.warn('Failed to load embedding cache:', error);
      }
    },
    
    getStats() {
      return { hits, misses, size: memoryCache.size };
    },
  };
}
```

éœ€è¦åœ¨ `lib/db/schema.ts` æ·»åŠ è¡¨å®šä¹‰:

```typescript
// åœ¨ç°æœ‰ schema ä¸­æ·»åŠ 
embeddingCache: '&key, embedding, accessTime',
```

---

### 3.7 æ·»åŠ  `findRelevantContent` API

**æ–°å»º `lib/ai/rag/find-relevant.ts`**:

```typescript
/**
 * Find Relevant Content Utility
 * 
 * High-level API for finding relevant content from embeddings
 */

import { embed, cosineSimilarity } from 'ai';
import type { EmbeddingModel } from 'ai';

export interface DocumentWithEmbedding {
  id: string;
  content: string;
  embedding: number[];
  metadata?: Record<string, unknown>;
}

export interface RelevantContent {
  id: string;
  content: string;
  similarity: number;
  metadata?: Record<string, unknown>;
}

export interface FindRelevantOptions {
  /** Embedding model to use for query */
  embeddingModel: EmbeddingModel<string>;
  /** Minimum similarity threshold (0-1) */
  similarityThreshold?: number;
  /** Maximum number of results */
  topK?: number;
  /** Maximum retries for embedding generation */
  maxRetries?: number;
}

/**
 * Find relevant content from a collection of documents
 * 
 * @example
 * ```typescript
 * const results = await findRelevantContent(
 *   'What is machine learning?',
 *   documents,
 *   {
 *     embeddingModel: openai.embedding('text-embedding-3-small'),
 *     similarityThreshold: 0.5,
 *     topK: 5,
 *   }
 * );
 * ```
 */
export async function findRelevantContent(
  query: string,
  documents: DocumentWithEmbedding[],
  options: FindRelevantOptions
): Promise<RelevantContent[]> {
  const { 
    embeddingModel, 
    similarityThreshold = 0.5, 
    topK = 5,
    maxRetries = 2,
  } = options;

  if (documents.length === 0) {
    return [];
  }

  // Generate query embedding
  const { embedding: queryEmbedding } = await embed({
    model: embeddingModel,
    value: query,
    maxRetries,
  });

  // Calculate similarities
  const results: RelevantContent[] = documents
    .map(doc => ({
      id: doc.id,
      content: doc.content,
      similarity: cosineSimilarity(queryEmbedding, doc.embedding),
      metadata: doc.metadata,
    }))
    .filter(r => r.similarity >= similarityThreshold)
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, topK);

  return results;
}

/**
 * Find relevant content using pre-computed query embedding
 */
export function findRelevantContentWithEmbedding(
  queryEmbedding: number[],
  documents: DocumentWithEmbedding[],
  options: {
    similarityThreshold?: number;
    topK?: number;
  } = {}
): RelevantContent[] {
  const { similarityThreshold = 0.5, topK = 5 } = options;

  return documents
    .map(doc => ({
      id: doc.id,
      content: doc.content,
      similarity: cosineSimilarity(queryEmbedding, doc.embedding),
      metadata: doc.metadata,
    }))
    .filter(r => r.similarity >= similarityThreshold)
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, topK);
}

/**
 * Batch find relevant content for multiple queries
 */
export async function batchFindRelevantContent(
  queries: string[],
  documents: DocumentWithEmbedding[],
  options: FindRelevantOptions
): Promise<Map<string, RelevantContent[]>> {
  const { embedMany } = await import('ai');
  const { 
    embeddingModel, 
    similarityThreshold = 0.5, 
    topK = 5,
    maxRetries = 2,
  } = options;

  // Generate all query embeddings at once
  const { embeddings: queryEmbeddings } = await embedMany({
    model: embeddingModel,
    values: queries,
    maxRetries,
  });

  // Find relevant content for each query
  const results = new Map<string, RelevantContent[]>();
  
  for (let i = 0; i < queries.length; i++) {
    const relevant = findRelevantContentWithEmbedding(
      queryEmbeddings[i],
      documents,
      { similarityThreshold, topK }
    );
    results.set(queries[i], relevant);
  }

  return results;
}
```

---

### 3.8 å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•

**ä¿®æ”¹ `lib/ai/embedding.ts`**:

```typescript
export interface EmbeddingConfig {
  // ... existing fields ...
  maxRetries?: number;        // AI SDK å†…ç½®æ”¯æŒï¼Œé»˜è®¤ 2
  abortSignal?: AbortSignal;  // æ”¯æŒå–æ¶ˆè¯·æ±‚
  onError?: (error: Error) => void;  // é”™è¯¯å›è°ƒ
}

export async function generateEmbedding(
  text: string,
  config: EmbeddingConfig
): Promise<EmbeddingResult> {
  // Check cache first
  if (config.cache) {
    const cacheKey = getCacheKey(text, config);
    const cached = config.cache.get(cacheKey);
    if (cached) {
      return { embedding: cached, usage: undefined };
    }
  }

  try {
    const model = getEmbeddingModel(config);

    const result = await embed({
      model,
      value: text,
      maxRetries: config.maxRetries ?? 2,      // ä½¿ç”¨ AI SDK å†…ç½®é‡è¯•
      abortSignal: config.abortSignal,         // æ”¯æŒå–æ¶ˆ
    });

    // Store in cache
    if (config.cache) {
      const cacheKey = getCacheKey(text, config);
      config.cache.set(cacheKey, result.embedding);
    }

    return {
      embedding: result.embedding,
      usage: result.usage ? { tokens: result.usage.tokens } : undefined,
    };
  } catch (error) {
    config.onError?.(error instanceof Error ? error : new Error(String(error)));
    throw error;
  }
}

export async function generateEmbeddings(
  texts: string[],
  config: EmbeddingConfig
): Promise<BatchEmbeddingResult> {
  // ... cache check logic ...

  try {
    const model = getEmbeddingModel(config);
    const result = await embedMany({
      model,
      values: textsToEmbed.map((t) => t.text),
      maxRetries: config.maxRetries ?? 2,
      maxParallelCalls: config.maxParallelCalls ?? 5,
      abortSignal: config.abortSignal,
    });

    // ... merge results and cache ...
    
    return {
      embeddings: results as number[][],
      usage: result.usage ? { tokens: result.usage.tokens } : undefined,
    };
  } catch (error) {
    config.onError?.(error instanceof Error ? error : new Error(String(error)));
    throw error;
  }
}
```

---

### 3.9 æ·»åŠ  `providerOptions` æ”¯æŒ

**ä¿®æ”¹ `lib/ai/embedding.ts`**:

```typescript
export interface EmbeddingConfig {
  // ... existing fields ...
  providerOptions?: {
    openai?: {
      dimensions?: number;
      user?: string;
    };
    google?: {
      outputDimensionality?: number;
      taskType?: 'RETRIEVAL_DOCUMENT' | 'RETRIEVAL_QUERY' | 'SEMANTIC_SIMILARITY' | 'CLASSIFICATION' | 'CLUSTERING';
    };
    cohere?: {
      inputType?: 'search_document' | 'search_query' | 'classification' | 'clustering';
      truncate?: 'NONE' | 'START' | 'END';
    };
    bedrock?: {
      dimensions?: number;
      normalize?: boolean;
    };
  };
}

export async function generateEmbedding(
  text: string,
  config: EmbeddingConfig
): Promise<EmbeddingResult> {
  // ... cache check ...

  const model = getEmbeddingModel(config);

  const result = await embed({
    model,
    value: text,
    maxRetries: config.maxRetries ?? 2,
    abortSignal: config.abortSignal,
    providerOptions: config.providerOptions,  // ä¼ é€’ provider ç‰¹å®šé€‰é¡¹
  });

  // ... cache and return ...
}
```

---

## 4. æ–‡ä»¶å˜æ›´æ¸…å•

### æ–°å»ºæ–‡ä»¶

| æ–‡ä»¶è·¯å¾„ | è¯´æ˜ |
|----------|------|
| `lib/ai/embedding-model-factory.ts` | Embedding æ¨¡å‹å·¥å‚ï¼Œæ”¯æŒé»˜è®¤é…ç½® |
| `lib/ai/embedding-cache.ts` | æŒä¹…åŒ–ç¼“å­˜å®ç° |
| `lib/ai/rag/rag-tools.ts` | RAG Tools å®šä¹‰ |
| `lib/ai/rag/find-relevant.ts` | é«˜çº§æŸ¥æ‰¾ API |
| `lib/ai/rag/rag-tools.test.ts` | å·¥å…·æµ‹è¯• |
| `lib/ai/rag/find-relevant.test.ts` | æŸ¥æ‰¾ API æµ‹è¯• |
| `lib/ai/embedding-cache.test.ts` | ç¼“å­˜æµ‹è¯• |

### ä¿®æ”¹æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„ | æ”¹åŠ¨è¯´æ˜ |
|----------|----------|
| `lib/ai/embedding.ts` | ä½¿ç”¨ AI SDK cosineSimilarityã€æ·»åŠ  providerã€å¢å¼ºé…ç½® |
| `lib/ai/rag/rag-pipeline.ts` | åˆ é™¤é‡å¤ cosineSimilarityï¼Œä½¿ç”¨å¯¼å…¥ |
| `lib/ai/rag/reranker.ts` | åˆ é™¤é‡å¤ cosineSimilarityï¼Œä½¿ç”¨å¯¼å…¥ |
| `lib/ai/rag/index.ts` | å¯¼å‡ºæ–°æ¨¡å— |
| `lib/vector/embedding.ts` | åŒæ­¥ç±»å‹æ›´æ–° |
| `hooks/use-rag.ts` | æ·»åŠ æ–°åŠŸèƒ½æ”¯æŒ |
| `lib/db/schema.ts` | æ·»åŠ  embeddingCache è¡¨ |

---

## 5. å®æ–½é¡ºåº

å»ºè®®æŒ‰ä»¥ä¸‹é¡ºåºå®æ–½ï¼š

### é˜¶æ®µ 1: åŸºç¡€æ”¹è¿› (ä½é£é™©)

1. âœ… ä½¿ç”¨ AI SDK åŸç”Ÿ `cosineSimilarity`
2. âœ… æ·»åŠ  `maxParallelCalls` æ”¯æŒ
3. âœ… å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

### é˜¶æ®µ 2: æ–°åŠŸèƒ½ (ä¸­ç­‰é£é™©)

1. æ–°å»º `findRelevantContent` API
2. æ–°å»º RAG Tools
3. æ·»åŠ æ›´å¤š embedding provider

### é˜¶æ®µ 3: é«˜çº§åŠŸèƒ½ (éœ€è¦æ›´å¤šæµ‹è¯•)

1. ä½¿ç”¨ `wrapEmbeddingModel` ä¸­é—´ä»¶
2. å®ç°æŒä¹…åŒ–ç¼“å­˜
3. æ·»åŠ  `providerOptions` æ”¯æŒ

### é˜¶æ®µ 4: æµ‹è¯•å’Œæ–‡æ¡£

1. æ›´æ–°æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
2. æ›´æ–°æ–‡æ¡£

---

## 6. ä¾èµ–å˜æ›´

```bash
# éœ€è¦å®‰è£…çš„æ–°ä¾èµ–
pnpm add @ai-sdk/amazon-bedrock @ai-sdk/azure

# å¯é€‰ç¤¾åŒº provider
pnpm add voyage-ai-provider
```

---

## 7. å‚è€ƒæ–‡æ¡£

- [AI SDK Embeddings](https://ai-sdk.dev/docs/ai-sdk-core/embeddings)
- [AI SDK RAG Chatbot Guide](https://ai-sdk.dev/cookbook/guides/rag-chatbot)
- [AI SDK Tools](https://ai-sdk.dev/docs/foundations/tools)
- [AI SDK Migration Guide 5.0](https://ai-sdk.dev/docs/migration-guides/migration-guide-5-0)
